<!--Copyright 2022 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with
the License. You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on
an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the

âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be
rendered properly in your Markdown viewer.

-->

# GPUæ¨ç†

GPU ä¸€ç›´æ˜¯æœºå™¨å­¦ä¹ çš„æ ‡å‡†ç¡¬ä»¶é€‰æ‹©ï¼Œä¸åŒäº CPUï¼Œå®ƒä»¬ä¼˜åŒ–äº†å†…å­˜å¸¦å®½å’Œå¹¶è¡Œæ€§ã€‚ä¸ºäº†è·Ÿä¸Šç°ä»£æ¨¡å‹çš„æ›´å¤§å°ºå¯¸æˆ–åœ¨ç°æœ‰å’Œè€æ—§ç¡¬ä»¶ä¸Šè¿è¡Œè¿™äº›å¤§å‹æ¨¡å‹ï¼Œæœ‰å‡ ç§ä¼˜åŒ–æ–¹æ³•å¯ä»¥åŠ é€Ÿ GPU æ¨ç†ã€‚åœ¨æœ¬æŒ‡å—ä¸­ï¼Œæ‚¨å°†å­¦ä¹ å¦‚ä½•ä½¿ç”¨ FlashAttention-2ï¼ˆæ›´èŠ‚çœå†…å­˜çš„æ³¨æ„åŠ›æœºåˆ¶ï¼‰ã€BetterTransformerï¼ˆPyTorch æœ¬åœ°å¿«é€Ÿè·¯å¾„æ‰§è¡Œï¼‰å’Œ bitsandbytes å°†æ‚¨çš„æ¨¡å‹é‡åŒ–åˆ°æ›´ä½çš„ç²¾åº¦ã€‚æœ€åï¼Œå­¦ä¹ å¦‚ä½•ä½¿ç”¨ ğŸ¤— Optimum é€šè¿‡ ONNX Runtime åœ¨ Nvidia GPU ä¸ŠåŠ é€Ÿæ¨ç†ã€‚

<Tip>

è¿™é‡Œæè¿°çš„å¤§å¤šæ•°ä¼˜åŒ–æ–¹æ³•åŒæ ·é€‚ç”¨äºå¤š GPU è®¾ç½®ï¼

</Tip>

## FlashAttention-2

<Tip>

FlashAttention-2 æ˜¯å®éªŒæ€§çš„ï¼Œæœªæ¥ç‰ˆæœ¬å¯èƒ½ä¼šå‘ç”Ÿç›¸å½“å¤§çš„æ›´æ”¹ã€‚

</Tip>

FlashAttention-2 æ˜¯æ ‡å‡†æ³¨æ„åŠ›æœºåˆ¶çš„æ›´å¿«ã€æ›´é«˜æ•ˆçš„å®ç°ï¼Œå¯ä»¥é€šè¿‡ä»¥ä¸‹æ–¹å¼æ˜¾è‘—åŠ é€Ÿæ¨ç†è¿‡ç¨‹ï¼š

1. åœ¨åºåˆ—é•¿åº¦ä¸Šè¿›è¡Œé¢å¤–çš„æ³¨æ„åŠ›è®¡ç®—å¹¶è¡ŒåŒ–
2. åœ¨ GPU çº¿ç¨‹ä¹‹é—´åˆ†é…å·¥ä½œï¼Œä»¥å‡å°‘å®ƒä»¬ä¹‹é—´çš„é€šä¿¡å’Œå…±äº«å†…å­˜è¯»/å†™

FlashAttention-2 æ”¯æŒ Llamaã€Mistral å’Œ Falcon æ¨¡å‹çš„æ¨ç†ã€‚æ‚¨å¯ä»¥é€šè¿‡åœ¨ GitHub ä¸Šæ‰“å¼€ Issue æˆ– Pull Request æ¥è¯·æ±‚æ·»åŠ å¯¹å¦ä¸€ä¸ªæ¨¡å‹çš„ FlashAttention-2 æ”¯æŒã€‚

åœ¨å¼€å§‹ä¹‹å‰ï¼Œè¯·ç¡®ä¿å·²å®‰è£… FlashAttention-2ï¼ˆæœ‰å…³æ›´å¤šå…ˆå†³æ¡ä»¶çš„è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…[å®‰è£…æŒ‡å—](https://github.com/Dao-AILab/flash-attention?tab=readme-ov-file#installation-and-features)ï¼‰ã€‚


```bash
pip install flash-attn --no-build-isolation
```

è¦å¯ç”¨ FlashAttention-2ï¼Œè¯·å‘ [`~AutoModelForCausalLM.from_pretrained`] æ·»åŠ  `use_flash_attention_2` å‚æ•°ï¼š

```python
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, LlamaForCausalLM

model_id = "tiiuae/falcon-7b"
tokenizer = AutoTokenizer.from_pretrained(model_id)

model = AutoModelForCausalLM.from_pretrained(
    model_id, 
    torch_dtype=torch.bfloat16, 
    use_flash_attention_2=True,
)
```

<Tip>

FlashAttention-2 åªèƒ½åœ¨æ¨¡å‹çš„ dtype ä¸º `fp16` æˆ– `bf16` æ—¶ä½¿ç”¨ï¼Œå¹¶ä¸”ä»…å¯åœ¨ Nvidia GPU ä¸Šè¿è¡Œã€‚åœ¨ä½¿ç”¨ FlashAttention-2 ä¹‹å‰ï¼Œè¯·ç¡®ä¿å°†æ‚¨çš„æ¨¡å‹è½¬æ¢ä¸ºç›¸åº”çš„ dtype å¹¶åŠ è½½åˆ°æ”¯æŒçš„è®¾å¤‡ä¸Šã€‚

</Tip>

FlashAttention-2 å¯ä»¥ä¸å…¶ä»–ä¼˜åŒ–æŠ€æœ¯ï¼ˆä¾‹å¦‚é‡åŒ–ï¼‰ç»“åˆï¼Œä»¥è¿›ä¸€æ­¥åŠ é€Ÿæ¨æ–­ã€‚ä¾‹å¦‚ï¼Œæ‚¨å¯ä»¥å°† FlashAttention-2 ä¸ 8 ä½æˆ– 4 ä½é‡åŒ–ç»“åˆä½¿ç”¨ï¼š

```py
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, LlamaForCausalLM

model_id = "tiiuae/falcon-7b"
tokenizer = AutoTokenizer.from_pretrained(model_id)

# load in 8bit
model = AutoModelForCausalLM.from_pretrained(
    model_id, 
    load_in_8bit=True,
    use_flash_attention_2=True,
)

# load in 4bit
model = AutoModelForCausalLM.from_pretrained(
    model_id, 
    load_in_4bit=True,
    use_flash_attention_2=True,
)
```

### é¢„æœŸåŠ é€Ÿ

ä½ å¯ä»¥ä»æ¨ç†é€Ÿåº¦çš„æ˜¾è‘—æå‡ä¸­å—ç›Šï¼Œå°¤å…¶æ˜¯åœ¨è¾“å…¥å…·æœ‰é•¿åºåˆ—çš„æƒ…å†µä¸‹ã€‚ç„¶è€Œï¼Œç”±äºFlashAttention-2ä¸æ”¯æŒä½¿ç”¨`padding tokens`è®¡ç®—æ³¨æ„åŠ›æƒé‡ï¼Œå› æ­¤åœ¨åºåˆ—åŒ…å«`padding tokens`æ—¶ï¼Œå¿…é¡»æ‰‹åŠ¨å¯¹æ‰¹é‡æ¨ç†çš„æ³¨æ„åŠ›æƒé‡è¿›è¡Œå¡«å……/è§£å¡«å……ã€‚è¿™ä¼šå¯¼è‡´ä½¿ç”¨`padding tokens`è¿›è¡Œæ‰¹é‡ç”Ÿæˆçš„é€Ÿåº¦æ˜æ˜¾å˜æ…¢ã€‚

ä¸ºäº†å…‹æœè¿™ä¸ªé—®é¢˜ï¼Œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­åº”è¯¥é¿å…åœ¨åºåˆ—ä¸­ä½¿ç”¨`padding tokens`ï¼ˆé€šè¿‡æ‰“åŒ…æ•°æ®é›†æˆ–[è¿æ¥åºåˆ—](https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_clm.py#L516)ç›´åˆ°è¾¾åˆ°æœ€å¤§åºåˆ—é•¿åº¦ï¼‰ã€‚

å¯¹äºåœ¨[tiiuae/falcon-7b](https://hf.co/tiiuae/falcon-7b)ä¸Šè¿›è¡Œçš„å•ä¸ªå‰å‘ä¼ é€’ï¼Œåºåˆ—é•¿åº¦ä¸º4096ä¸”ä¸ä½¿ç”¨`padding tokens`çš„å„ç§æ‰¹é‡å¤§å°ä¸‹ï¼Œé¢„æœŸçš„åŠ é€Ÿæ•ˆæœæ˜¯ï¼š

<div style="text-align: center">
<img src="https://huggingface.co/datasets/ybelkada/documentation-images/resolve/main/falcon-7b-inference-large-seqlen.png">
</div>

å¯¹äºåœ¨ [meta-llama/Llama-7b-hf](https://hf.co/meta-llama/Llama-7b-hf) æ¨¡å‹ä¸Šçš„å•æ¬¡å‰å‘ä¼ é€’ï¼Œå¸¦æœ‰é•¿åº¦ä¸º 4096 çš„åºåˆ—å’Œå„ç§ä¸å¸¦`padding tokens`çš„æ‰¹é‡å¤§å°ï¼Œé¢„æœŸçš„åŠ é€Ÿæ•ˆæœä¸ºï¼š

<div style="text-align: center">
<img src="https://huggingface.co/datasets/ybelkada/documentation-images/resolve/main/llama-7b-inference-large-seqlen.png">
</div>

å¯¹äºåŒ…å«`padding tokens`çš„åºåˆ—ï¼ˆä½¿ç”¨`padding tokens`è¿›è¡Œç”Ÿæˆï¼‰ï¼Œæ‚¨éœ€è¦å–æ¶ˆ/å¡«å……è¾“å…¥åºåˆ—ï¼Œä»¥æ­£ç¡®è®¡ç®—æ³¨æ„åŠ›æƒé‡ã€‚å¯¹äºç›¸å¯¹è¾ƒå°çš„åºåˆ—é•¿åº¦ï¼Œå•æ¬¡å‰å‘ä¼ é€’ä¼šäº§ç”Ÿä¸€äº›é¢å¤–å¼€é”€ï¼Œå¯¼è‡´è½»å¾®çš„åŠ é€Ÿæ•ˆæœï¼ˆåœ¨ä¸‹é¢çš„ç¤ºä¾‹ä¸­ï¼Œè¾“å…¥çš„ 30% å¡«å……æœ‰`padding tokens`ï¼‰ã€‚

<div style="text-align: center">
<img src="https://huggingface.co/datasets/ybelkada/documentation-images/resolve/main/llama-2-small-seqlen-padding.png">
</div>

ä½†å¯¹äºæ›´å¤§çš„åºåˆ—é•¿åº¦ï¼Œæ‚¨å¯ä»¥æœŸå¾…æ›´å¤šçš„åŠ é€Ÿä¼˜åŠ¿ï¼š

<Tip>

FlashAttentionæ›´èŠ‚çœå†…å­˜ï¼Œè¿™æ„å‘³ç€æ‚¨å¯ä»¥åœ¨æ›´å¤§çš„åºåˆ—é•¿åº¦ä¸Šè¿›è¡Œè®­ç»ƒï¼Œè€Œæ— éœ€æ‹…å¿ƒå†…å­˜ä¸è¶³çš„é—®é¢˜ã€‚å¯¹äºæ›´å¤§çš„åºåˆ—é•¿åº¦ï¼Œæ‚¨å¯èƒ½å°†å†…å­˜ä½¿ç”¨é™ä½å¤šè¾¾20å€ã€‚æŸ¥çœ‹[flash-attention](https://github.com/Dao-AILab/flash-attention)ä»“åº“è·å–æ›´å¤šè¯¦æƒ…ã€‚

</Tip>

<div style="text-align: center">
<img src="https://huggingface.co/datasets/ybelkada/documentation-images/resolve/main/llama-2-large-seqlen-padding.png">
</div>

## BetterTransformer

<Tip>

è¯·æŸ¥é˜…æˆ‘ä»¬åœ¨[PyTorch 2.0ä¸­ğŸ¤—è§£ç å™¨æ¨¡å‹çš„å¼€ç®±å³ç”¨åŠ é€Ÿå’Œå†…å­˜èŠ‚çœ](https://pytorch.org/blog/out-of-the-box-acceleration/)ä¸­çš„BetterTransformerå’Œscaled dot product attentionçš„åŸºå‡†æµ‹è¯•ï¼Œä»¥åŠåœ¨[BetterTransformer](https://medium.com/pytorch/bettertransformer-out-of-the-box-performance-for-huggingface-transformers-3fbe27d50ab2)åšæ–‡ä¸­äº†è§£æ›´å¤šæœ‰å…³å¿«é€Ÿè·¯å¾„æ‰§è¡Œçš„ä¿¡æ¯ã€‚

</Tip>

BetterTransformeråŠ é€Ÿæ¨ç†è¿‡ç¨‹ï¼Œé‡‡ç”¨å¿«é€Ÿè·¯å¾„æ‰§è¡Œï¼ˆTransformerå‡½æ•°çš„åŸç”ŸPyTorchä¸“ç”¨å®ç°ï¼‰ã€‚å¿«é€Ÿè·¯å¾„æ‰§è¡Œä¸­çš„ä¸¤é¡¹ä¼˜åŒ–æªæ–½åŒ…æ‹¬ï¼š

1. èåˆï¼šå°†å¤šä¸ªé¡ºåºæ“ä½œåˆå¹¶æˆå•ä¸ªâ€œå†…æ ¸â€ï¼Œå‡å°‘è®¡ç®—æ­¥éª¤æ¬¡æ•°ã€‚
2. è·³è¿‡å†…åœ¨çš„`padding tokens`ç¨€ç–æ€§ï¼Œä»¥é¿å…å’Œ`nested tensors`ä¸­è¿›è¡Œä¸å¿…è¦çš„è®¡ç®—ã€‚

BetterTransformerè¿˜å°†æ‰€æœ‰æ³¨æ„åŠ›æ“ä½œè½¬æ¢ä¸ºæ›´èŠ‚çº¦å†…å­˜çš„[scaled dot product attentionï¼ˆSDPAï¼‰](https://pytorch.org/docs/master/generated/torch.nn.functional.scaled_dot_product_attention)ï¼Œå¹¶åœ¨å†…éƒ¨è°ƒç”¨ä¼˜åŒ–çš„å†…æ ¸ï¼Œä¾‹å¦‚[FlashAttention](https://huggingface.co/papers/2205.14135)ã€‚

åœ¨å¼€å§‹ä¹‹å‰ï¼Œè¯·ç¡®ä¿å·²[å®‰è£…ğŸ¤— Optimum](https://huggingface.co/docs/optimum/installation)ã€‚

ç„¶åï¼Œæ‚¨å¯ä»¥ä½¿ç”¨ [`PreTrainedModel.to_bettertransformer`] æ–¹æ³•å¯ç”¨BetterTransformerï¼š

```python
model = model.to_bettertransformer()
```

æ‚¨å¯ä»¥ä½¿ç”¨ [`~PreTrainedModel.reverse_bettertransformer`] æ–¹æ³•è¿˜åŸä¸ºåŸå§‹çš„Transformersæ¨¡å‹ã€‚åœ¨ä¿å­˜æ¨¡å‹ä»¥ä½¿ç”¨ä¼ ç»Ÿçš„Transformerså»ºæ¨¡ä¹‹å‰ï¼Œåº”è¯¥ä½¿ç”¨è¿™ç§æ–¹æ³•ï¼š

```py
model = model.reverse_bettertransformer()
model.save_pretrained("saved_model")
```

### FlashAttention

SDPAåº•å±‚ä¹Ÿå¯ä»¥è°ƒç”¨FlashAttentionå†…æ ¸ã€‚FlashAttentionä»…é€‚ç”¨äºä½¿ç”¨ `fp16` æˆ– `bf16` dtype çš„æ¨¡å‹ï¼Œå› æ­¤åœ¨ä½¿ç”¨ä¹‹å‰ï¼Œè¯·ç¡®ä¿å°†æ¨¡å‹è½¬æ¢ä¸ºé€‚å½“çš„dtypeã€‚

è¦å¯ç”¨FlashAttentionï¼Œæˆ–è€…æ£€æŸ¥å®ƒåœ¨ç»™å®šç¯å¢ƒï¼ˆç¡¬ä»¶ã€é—®é¢˜è§„æ¨¡ï¼‰ä¸­æ˜¯å¦å¯ç”¨ï¼Œè¯·ä½¿ç”¨[`torch.backends.cuda.sdp_kernel`](https://pytorch.org/docs/master/backends.html#torch.backends.cuda.sdp_kernel) ä½œä¸ºä¸Šä¸‹æ–‡ç®¡ç†å™¨ï¼š

```diff
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("facebook/opt-350m")
model = AutoModelForCausalLM.from_pretrained("facebook/opt-350m", torch_dtype=torch.float16).to("cuda")
# convert the model to BetterTransformer
model.to_bettertransformer()

input_text = "Hello my dog is cute and"
inputs = tokenizer(input_text, return_tensors="pt").to("cuda")

+ with torch.backends.cuda.sdp_kernel(enable_flash=True, enable_math=False, enable_mem_efficient=False):
    outputs = model.generate(**inputs)

print(tokenizer.decode(outputs[0], skip_special_tokens=True))
```

å¦‚æœæ‚¨é‡åˆ°ä»¥ä¸‹çš„bugå¹¶ä¸”é”™è¯¯è¿½è¸ªä¿¡æ¯ï¼Œè¯·å°è¯•ä½¿ç”¨PyTorchçš„nightlyç‰ˆæœ¬ï¼Œå®ƒå¯èƒ½å¯¹FlashAttentionæœ‰æ›´å¹¿æ³›çš„è¦†ç›–ã€‚

```bash
RuntimeError: No available kernel. Aborting execution.

# install PyTorch nightly
pip3 install -U --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cu118
```

## bitsandbytes

bitsandbytes æ˜¯ä¸€ä¸ªé‡åŒ–åº“ï¼Œæ”¯æŒ 4 ä½å’Œ 8 ä½çš„é‡åŒ–ã€‚é‡åŒ–ç›¸å¯¹äºå®Œæ•´ç²¾åº¦ç‰ˆæœ¬ï¼Œå¯ä»¥å‡å°æ¨¡å‹å¤§å°ï¼Œä½¿å¤§æ¨¡å‹æ›´å®¹æ˜“é€‚é…äºå†…å­˜æœ‰é™çš„ GPUã€‚

ç¡®ä¿æ‚¨å·²å®‰è£… bitsnbytes å’Œ ğŸ¤— Accelerateã€‚

```bash
# these versions support 8-bit and 4-bit
pip install bitsandbytes>=0.39.0 accelerate>=0.20.0

# install Transformers
pip install transformers
```

### 4-bit

è¦åŠ è½½ä¸€ä¸ª 4 ä½çš„æ¨¡å‹è¿›è¡Œæ¨ç†ï¼Œä½¿ç”¨ `load_in_4bit` å‚æ•°ã€‚`device_map` å‚æ•°æ˜¯å¯é€‰çš„ï¼Œä½†æˆ‘ä»¬å»ºè®®å°†å…¶è®¾ç½®ä¸º `"auto"`ï¼Œä»¥å…è®¸ ğŸ¤— Accelerate æ ¹æ®ç¯å¢ƒä¸­çš„å¯ç”¨èµ„æºè‡ªåŠ¨é«˜æ•ˆåœ°åˆ†é…æ¨¡å‹ã€‚

```py
from transformers import AutoModelForCausalLM

model_name = "bigscience/bloom-2b5"
model_4bit = AutoModelForCausalLM.from_pretrained(model_name, device_map="auto", load_in_4bit=True)
```

åŠ è½½ä¸€ä¸ª 4 ä½æ¨¡å‹ä»¥åœ¨å¤šä¸ª GPU ä¸Šè¿›è¡Œæ¨ç†ï¼Œæ‚¨å¯ä»¥æ§åˆ¶ä¸ºæ¯ä¸ª GPU åˆ†é…å¤šå°‘å†…å­˜ã€‚ä¾‹å¦‚ï¼Œè¦ä¸ºç¬¬ä¸€ä¸ª GPU åˆ†é… 600MB çš„å†…å­˜ï¼Œä¸ºç¬¬äºŒä¸ª GPU åˆ†é… 1GB çš„å†…å­˜ï¼š

```py
max_memory_mapping = {0: "600MB", 1: "1GB"}
model_name = "bigscience/bloom-3b"
model_4bit = AutoModelForCausalLM.from_pretrained(
    model_name, device_map="auto", load_in_4bit=True, max_memory=max_memory_mapping
)
```

### 8-bit

<Tip>

å¦‚æœæ‚¨å¯¹ 8 ä½é‡åŒ–çš„æ¦‚å¿µæ„Ÿå…´è¶£ï¼Œå¯ä»¥é˜…è¯»[ä½¿ç”¨Hugging Face Transformersã€Accelerateå’Œbitsandbytesè¿›è¡Œå¤§è§„æ¨¡transformerçš„8ä½çŸ©é˜µä¹˜æ³•çš„ç®€ä»‹](https://huggingface.co/blog/hf-bitsandbytes-integration)çš„åšå®¢æ–‡ç« ã€‚

</Tip>

è¦åŠ è½½ä¸€ä¸ª 8 ä½æ¨ç†æ¨¡å‹ï¼Œè¯·ä½¿ç”¨ `load_in_8bit` å‚æ•°ã€‚`device_map` å‚æ•°æ˜¯å¯é€‰çš„ï¼Œä½†æˆ‘ä»¬å»ºè®®å°†å…¶è®¾ç½®ä¸º `"auto"`ï¼Œä»¥ä¾¿ ğŸ¤— Accelerate æ ¹æ®ç¯å¢ƒä¸­å¯ç”¨çš„èµ„æºè‡ªåŠ¨ä¸”é«˜æ•ˆåœ°åˆ†é…æ¨¡å‹ï¼š

```py
from transformers import AutoModelForCausalLM

model_name = "bigscience/bloom-2b5"
model_8bit = AutoModelForCausalLM.from_pretrained(model_name, device_map="auto", load_in_8bit=True)
```

å¦‚æœè¦ä½¿ç”¨ 8 ä½æ¨¡å‹è¿›è¡Œæ–‡æœ¬ç”Ÿæˆï¼Œåº”ä½¿ç”¨ [`~transformers.GenerationMixin.generate`] æ–¹æ³•ï¼Œè€Œä¸æ˜¯ [`Pipeline`] å‡½æ•°ï¼Œå› ä¸ºåè€…å¯¹ 8 ä½æ¨¡å‹æœªè¿›è¡Œä¼˜åŒ–ï¼Œé€Ÿåº¦ä¼šæ…¢ä¸€äº›ã€‚æŸäº›é‡‡æ ·ç­–ç•¥ï¼Œæ¯”å¦‚ nucleus é‡‡æ ·ï¼Œå¯¹ 8 ä½æ¨¡å‹ä¹Ÿä¸å— [`Pipeline`] æ”¯æŒã€‚åŒæ—¶ï¼Œè¾“å…¥åº”æ”¾åœ¨ä¸æ¨¡å‹ç›¸åŒçš„è®¾å¤‡ä¸Šï¼š

```py
from transformers import AutoModelForCausalLM, AutoTokenizer

model_name = "bigscience/bloom-2b5"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model_8bit = AutoModelForCausalLM.from_pretrained(model_name, device_map="auto", load_in_8bit=True)

prompt = "Hello, my llama is cute"
inputs = tokenizer(prompt, return_tensors="pt").to("cuda")
generated_ids = model.generate(**inputs)
outputs = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)
```

åŠ è½½å¤šä¸ª GPU ä¸­çš„ 4 ä½æ¨ç†æ¨¡å‹æ—¶ï¼Œå¯ä»¥æ§åˆ¶è¦åˆ†é…ç»™æ¯ä¸ª GPU çš„æ˜¾å­˜é‡ã€‚ä¾‹å¦‚ï¼Œåˆ†é… 1GB æ˜¾å­˜ç»™ç¬¬ä¸€ä¸ª GPUï¼Œåˆ†é… 2GB æ˜¾å­˜ç»™ç¬¬äºŒä¸ª GPUï¼š

```py
max_memory_mapping = {0: "1GB", 1: "2GB"}
model_name = "bigscience/bloom-3b"
model_8bit = AutoModelForCausalLM.from_pretrained(
    model_name, device_map="auto", load_in_8bit=True, max_memory=max_memory_mapping
)
```

<Tip>

è¯·éšæ„åœ¨ Google Colab çš„å…è´¹ GPU ä¸Šå°è¯•è¿è¡Œä¸€ä¸ªæ‹¥æœ‰ 110 äº¿å‚æ•°çš„ [T5 æ¨¡å‹](https://colab.research.google.com/drive/1YORPWx4okIHXnjW7MSAidXN29mPVNT7F?usp=sharing) æˆ–è€… 30 äº¿å‚æ•°çš„ [BLOOM æ¨¡å‹](https://colab.research.google.com/drive/1qOjXfQIAULfKvZqwCen8-MoWKGdSatZ4?usp=sharing) è¿›è¡Œæ¨ç†ï¼

</Tip>

## ğŸ¤— Optimum

<Tip>

å­¦ä¹ æœ‰å…³åœ¨ [NVIDIA GPU ä¸Šçš„åŠ é€Ÿæ¨ç†](https://huggingface.co/docs/optimum/onnxruntime/usage_guides/gpu#accelerated-inference-on-nvidia-gpus) çš„æœ‰å…³ä½¿ç”¨ ORT ä¸ ğŸ¤— Optimum çš„è¯¦ç»†ä¿¡æ¯ã€‚æ­¤éƒ¨åˆ†ä»…æä¾›ç®€å•çš„ä¾‹å­ã€‚

</Tip>

ONNX Runtimeï¼ˆORTï¼‰æ˜¯ä¸€ä¸ªæ¨¡å‹åŠ é€Ÿå™¨ï¼Œæ”¯æŒåœ¨ Nvidia GPU ä¸Šè¿›è¡ŒåŠ é€Ÿæ¨ç†ã€‚ORTä½¿ç”¨ä¼˜åŒ–æŠ€æœ¯ï¼Œä¾‹å¦‚å°†å¸¸è§æ“ä½œèåˆä¸ºå•ä¸ªèŠ‚ç‚¹å’Œå¸¸æ•°æŠ˜å ï¼Œä»¥å‡å°‘æ‰§è¡Œçš„è®¡ç®—æ•°é‡ï¼Œä»è€ŒåŠ å¿«æ¨ç†é€Ÿåº¦ã€‚æ­¤å¤–ï¼ŒORTä¼šå°†è®¡ç®—é‡æœ€å¤§çš„æ“ä½œæ”¾åœ¨ GPU ä¸Šï¼Œå°†å…¶ä»–æ“ä½œæ”¾åœ¨ CPU ä¸Šï¼Œæ™ºèƒ½åœ°åœ¨ä¸¤ä¸ªè®¾å¤‡ä¹‹é—´åˆ†é…å·¥ä½œè´Ÿè½½ã€‚

ORTå—åˆ° ğŸ¤— Optimum çš„æ”¯æŒï¼Œå¯ä»¥åœ¨ ğŸ¤— Transformers ä¸­ä½¿ç”¨ã€‚æ‚¨éœ€è¦ä½¿ç”¨ [`~optimum.onnxruntime.ORTModel`] å¤„ç†æ‚¨çš„ä»»åŠ¡ï¼Œå¹¶æŒ‡å®š `provider` å‚æ•°ï¼Œå¯ä»¥è®¾ç½®ä¸º [`CUDAExecutionProvider`](https://huggingface.co/docs/optimum/onnxruntime/usage_guides/gpu#cudaexecutionprovider) æˆ– [`TensorrtExecutionProvider`](https://huggingface.co/docs/optimum/onnxruntime/usage_guides/gpu#tensorrtexecutionprovider)ã€‚å¦‚æœè¦åŠ è½½å°šæœªå¯¼å‡ºä¸º ONNX æ ¼å¼çš„æ¨¡å‹ï¼Œå¯ä»¥å°† `export=True` è®¾ç½®ä¸ºå°†æ¨¡å‹å³æ—¶è½¬æ¢ä¸º ONNX æ ¼å¼ï¼š


```py
from optimum.onnxruntime import ORTModelForSequenceClassification

ort_model = ORTModelForSequenceClassification.from_pretrained(
  "distilbert-base-uncased-finetuned-sst-2-english",
  export=True,
  provider="CUDAExecutionProvider",
)
```

ç°åœ¨æ‚¨å¯ä»¥ä½¿ç”¨è¯¥æ¨¡å‹è¿›è¡Œæ¨ç†ï¼š

```py
from optimum.pipelines import pipeline
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("distilbert-base-uncased-finetuned-sst-2-english")

pipeline = pipeline(task="text-classification", model=ort_model, tokenizer=tokenizer, device="cuda:0")
result = pipeline("Both the music and visual were astounding, not to mention the actors performance.")
```

## ç»„åˆä¼˜åŒ–

é€šå¸¸å¯ä»¥å°†ä¸Šè¿°å‡ ç§ä¼˜åŒ–æŠ€æœ¯ç»„åˆèµ·æ¥ï¼Œä»¥è·å¾—æ¨¡å‹çš„æœ€ä½³æ¨ç†æ€§èƒ½ã€‚ä¾‹å¦‚ï¼Œæ‚¨å¯ä»¥ä»¥4ä½åŠ è½½æ¨¡å‹ï¼Œç„¶åä½¿ç”¨FlashAttentionå¯ç”¨BetterTransformer :

```py
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig

# load model in 4-bit
quantization_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.float16
)

tokenizer = AutoTokenizer.from_pretrained("facebook/opt-350m")
model = AutoModelForCausalLM.from_pretrained("facebook/opt-350m", quantization_config=quantization_config)

# enable BetterTransformer
model = model.to_bettertransformer()

input_text = "Hello my dog is cute and"
inputs = tokenizer(input_text, return_tensors="pt").to("cuda")

# enable FlashAttention
with torch.backends.cuda.sdp_kernel(enable_flash=True, enable_math=False, enable_mem_efficient=False):
    outputs = model.generate(**inputs)

print(tokenizer.decode(outputs[0], skip_special_tokens=True))
```
