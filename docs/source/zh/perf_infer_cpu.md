<!--Copyright 2022 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with
the License. You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on
an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the

âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be
rendered properly in your Markdown viewer.

-->

# CPUæ¨ç†

é€šè¿‡ä¸€äº›ä¼˜åŒ–æŠ€æœ¯ï¼Œå¯ä»¥åœ¨ CPU ä¸Šé«˜æ•ˆåœ°è¿è¡Œå¤§å‹æ¨¡å‹æ¨ç†ã€‚å…¶ä¸­ä¸€ç§ä¼˜åŒ–æŠ€æœ¯æ¶‰åŠå°† PyTorch ä»£ç ç¼–è¯‘æˆé«˜æ€§èƒ½ç¯å¢ƒï¼ˆå¦‚ C++ï¼‰çš„ä¸­é—´æ ¼å¼ã€‚å¦ä¸€ç§æŠ€æœ¯æ˜¯å°†å¤šä¸ªæ“ä½œèåˆä¸ºä¸€ä¸ªå†…æ ¸ï¼Œä»¥å‡å°‘å•ç‹¬è¿è¡Œæ¯ä¸ªæ“ä½œçš„å¼€é”€ã€‚

æ‚¨å¯ä»¥å­¦ä¹ å¦‚ä½•ä½¿ç”¨ [BetterTransformer](https://pytorch.org/blog/a-better-transformer-for-fast-transformer-encoder-inference/) è¿›è¡Œæ›´å¿«çš„æ¨ç†ï¼Œå¹¶äº†è§£å¦‚ä½•å°† PyTorch ä»£ç è½¬æ¢ä¸º [TorchScript](https://pytorch.org/tutorials/beginner/Intro_to_TorchScript_tutorial.html)ã€‚å¦‚æœæ‚¨æ­£åœ¨ä½¿ç”¨è‹±ç‰¹å°” CPUï¼Œæ‚¨è¿˜å¯ä»¥ä½¿ç”¨ [Intel Extension for PyTorch](https://intel.github.io/intel-extension-for-pytorch/cpu/latest/index.html) ä¸­çš„ [å›¾ä¼˜åŒ–](https://intel.github.io/intel-extension-for-pytorch/cpu/latest/tutorials/features.html#graph-optimization) æ¥è¿›ä¸€æ­¥æé«˜æ¨ç†é€Ÿåº¦ã€‚æœ€åï¼Œå­¦ä¹ å¦‚ä½•ä½¿ç”¨ ğŸ¤— Optimum é€šè¿‡ ONNX Runtime æˆ– OpenVINOï¼ˆå¦‚æœæ‚¨æ­£åœ¨ä½¿ç”¨è‹±ç‰¹å°” CPUï¼‰æ¥åŠ é€Ÿæ¨ç†ã€‚

## BetterTransformer

BetterTransformer é€šè¿‡å…¶å¿«é€Ÿè·¯å¾„ï¼ˆTransformer å‡½æ•°çš„åŸç”Ÿ PyTorch ä¸“ç”¨å®ç°ï¼‰åŠ é€Ÿæ¨ç†ã€‚å¿«é€Ÿè·¯å¾„æ‰§è¡Œçš„ä¸¤ä¸ªä¼˜åŒ–æ˜¯ï¼š

1. èåˆï¼ˆfusionï¼‰ï¼Œå°†å¤šä¸ªé¡ºåºæ“ä½œåˆå¹¶ä¸ºå•ä¸ªâ€œæ ¸å¿ƒâ€ï¼Œä»¥å‡å°‘è®¡ç®—æ­¥éª¤çš„æ•°é‡
2. è·³è¿‡`padding tokens`çš„å›ºæœ‰ç¨€ç–æ€§ï¼Œä»¥é¿å…ä¸`nested tensors`çš„ä¸å¿…è¦è®¡ç®—

BetterTransformer è¿˜å°†æ‰€æœ‰attentionæ“ä½œè½¬æ¢ä¸ºæ›´èŠ‚çœå†…å­˜çš„ [scaled dot product attention](https://pytorch.org/docs/master/generated/torch.nn.functional.scaled_dot_product_attention)ã€‚

<Tip>

BetterTransformer å¹¶éé€‚ç”¨äºæ‰€æœ‰æ¨¡å‹ã€‚è¯·æ£€æŸ¥æ­¤ [åˆ—è¡¨](https://huggingface.co/docs/optimum/bettertransformer/overview#supported-models)ï¼ŒæŸ¥çœ‹æ¨¡å‹æ˜¯å¦æ”¯æŒ BetterTransformerã€‚


</Tip>

åœ¨å¼€å§‹ä¹‹å‰ï¼Œè¯·ç¡®ä¿å·²å®‰è£… ğŸ¤— Optimum [installed](https://huggingface.co/docs/optimum/installation)ã€‚

ä½¿ç”¨ [`PreTrainedModel.to_bettertransformer`] æ–¹æ³•å¯ç”¨ BetterTransformerï¼š

```py
from transformers import AutoModelForCausalLM

model = AutoModelForCausalLM.from_pretrained("bigcode/starcoder")
model.to_bettertransformer()
```

## TorchScript

TorchScript æ˜¯ PyTorch çš„ä¸­é—´æ¨¡å‹è¡¨ç¤ºå½¢å¼ï¼Œå¯åœ¨å¯¹æ€§èƒ½è¦æ±‚å¾ˆé«˜çš„ç”Ÿäº§ç¯å¢ƒä¸­è¿è¡Œã€‚æ‚¨å¯ä»¥åœ¨ PyTorch ä¸­è®­ç»ƒæ¨¡å‹ï¼Œç„¶åå°†å…¶å¯¼å‡ºåˆ° TorchScript ä»¥æ‘†è„±æ¨¡å‹åœ¨ Python æ€§èƒ½ä¸Šçš„é™åˆ¶ã€‚PyTorch [traces](https://pytorch.org/docs/stable/generated/torch.jit.trace.html) æ¨¡å‹ä»¥è¿”å›ä¸€ä¸ª [`ScriptFunction`]ï¼Œå®ƒé€šè¿‡å³æ—¶ç¼–è¯‘ï¼ˆJITï¼‰è¿›è¡Œä¼˜åŒ–ã€‚ä¸é»˜è®¤çš„å³æ—¶æ¨¡å¼ç›¸æ¯”ï¼ŒPyTorch ä¸­çš„ JIT æ¨¡å¼é€šå¸¸é€šè¿‡æ“ä½œèåˆç­‰ä¼˜åŒ–æŠ€æœ¯è·å¾—æ›´å¥½çš„æ¨æ–­æ€§èƒ½ã€‚

è¦äº†è§£ TorchScript çš„åŸºç¡€çŸ¥è¯†ï¼Œè¯·å‚é˜… [Introduction to PyTorch TorchScript](https://pytorch.org/tutorials/beginner/Intro_to_TorchScript_tutorial.html) æ•™ç¨‹ã€‚

ä½¿ç”¨ [`Trainer`] ç±»ï¼Œæ‚¨å¯ä»¥é€šè¿‡è®¾ç½® `--jit_mode_eval` æ ‡å¿—ä¸º CPU æ¨æ–­å¯ç”¨ JIT æ¨¡å¼ï¼š

```bash
python run_qa.py \
--model_name_or_path csarron/bert-base-uncased-squad-v1 \
--dataset_name squad \
--do_eval \
--max_seq_length 384 \
--doc_stride 128 \
--output_dir /tmp/ \
--no_cuda \
--jit_mode_eval
```

<Tip warning={true}>

å¯¹äº PyTorch >= 1.14.0ï¼ŒJIT æ¨¡å¼å¯¹äºé¢„æµ‹å’Œè¯„ä¼°ä»»ä½•æ¨¡å‹éƒ½å¯èƒ½æœ‰ç›Šï¼Œå› ä¸º `jit.trace` æ”¯æŒå­—å…¸è¾“å…¥ã€‚

å¯¹äº PyTorch < 1.14.0ï¼Œå¦‚æœæ¨¡å‹çš„å‰å‘å‚æ•°é¡ºåºä¸ `jit.trace` ä¸­çš„å…ƒç»„è¾“å…¥é¡ºåºåŒ¹é…ï¼Œä¾‹å¦‚é—®ç­”æ¨¡å‹ï¼Œé‚£ä¹ˆ JIT æ¨¡å¼å¯èƒ½æœ‰ç›Šã€‚å¦‚æœæ¨¡å‹çš„å‰å‘å‚æ•°é¡ºåºä¸ `jit.trace` ä¸­çš„å…ƒç»„è¾“å…¥é¡ºåºä¸åŒ¹é…ï¼Œä¾‹å¦‚æ–‡æœ¬åˆ†ç±»æ¨¡å‹ï¼Œ`jit.trace` å°†å¤±è´¥ï¼Œå¹¶ä¸”æˆ‘ä»¬ä¼šæ•è·è¿™ä¸ªå¼‚å¸¸æ¥è¿›è¡Œå›é€€ã€‚æˆ‘ä»¬ä½¿ç”¨æ—¥å¿—è®°å½•æ¥é€šçŸ¥ç”¨æˆ·ã€‚

</Tip>

## IPEXå›¾ä¼˜åŒ–

IntelÂ® Extension for PyTorchï¼ˆIPEXï¼‰ä¸º Intel CPU æä¾› JIT æ¨¡å¼ä¸‹çš„è¿›ä¸€æ­¥ä¼˜åŒ–ï¼Œå¹¶å»ºè®®å°†å…¶ä¸ TorchScript ç»“åˆä½¿ç”¨ä»¥è·å¾—æ›´å¿«çš„æ€§èƒ½ã€‚IPEX çš„å›¾ä¼˜åŒ–å¯ä»¥åˆå¹¶è¯¸å¦‚å¤šå¤´æ³¨æ„åŠ›ã€Concat Linearã€Linear + Addã€Linear + Geluã€Add + LayerNorm ç­‰æ“ä½œã€‚

è¦å……åˆ†åˆ©ç”¨è¿™äº›å›¾ä¼˜åŒ–ï¼Œè¯·ç¡®ä¿å·²ç»[å®‰è£…](https://intel.github.io/intel-extension-for-pytorch/cpu/latest/tutorials/installation.html)äº† IPEXã€‚

```bash
pip install intel_extension_for_pytorch
```

åœ¨ `Trainer` ç±»ä¸­è®¾ç½® `--use_ipex` å’Œ `--jit_mode_eval` æ ‡å¿—ï¼Œä»¥å¯ç”¨å¸¦å›¾ä¼˜åŒ–çš„ JIT æ¨¡å¼ï¼š

```bash
python run_qa.py \
--model_name_or_path csarron/bert-base-uncased-squad-v1 \
--dataset_name squad \
--do_eval \
--max_seq_length 384 \
--doc_stride 128 \
--output_dir /tmp/ \
--no_cuda \
--use_ipex \
--jit_mode_eval
```

## ğŸ¤— Optimum

<Tip>

äº†è§£å¦‚ä½•åœ¨[Optimum Inference with ONNX Runtime](https://huggingface.co/docs/optimum/onnxruntime/usage_guides/models)æŒ‡å—ä¸­ä½¿ç”¨ORTå’ŒğŸ¤— Optimumã€‚è¿™éƒ¨åˆ†ä»…æä¾›äº†ç®€è¦çš„ç¤ºä¾‹ã€‚

</Tip>

ONNX Runtime (ORT)æ˜¯ä¸€ä¸ªæ¨¡å‹åŠ é€Ÿå™¨ï¼Œé»˜è®¤åœ¨ CPU ä¸Šè¿è¡Œæ¨ç†ã€‚ORT å—åˆ°ğŸ¤— Optimum çš„æ”¯æŒï¼Œå¯ä»¥åœ¨ ğŸ¤— Transformers ä¸­ä½¿ç”¨ï¼Œè€Œæ— éœ€å¯¹ä»£ç è¿›è¡Œå¤ªå¤šæ›´æ”¹ã€‚æ‚¨åªéœ€è¦ç”¨å…¶ç­‰ä»·çš„ [`~optimum.onnxruntime.ORTModel`] æ›¿æ¢ğŸ¤— Transformersçš„ `AutoClass`ï¼Œå¹¶åŠ è½½ ONNX æ ¼å¼çš„æ£€æŸ¥ç‚¹ã€‚

ä¾‹å¦‚ï¼Œå¦‚æœæ‚¨è¦åœ¨é—®ç­”ä»»åŠ¡ä¸Šè¿›è¡Œæ¨ç†ï¼ŒåŠ è½½ [optimum/roberta-base-squad2](https://huggingface.co/optimum/roberta-base-squad2) checkpointï¼Œå…¶ä¸­åŒ…å«ä¸€ä¸ª `model.onnx` æ–‡ä»¶ï¼š

```py
from transformers import AutoTokenizer, pipeline
from optimum.onnxruntime import ORTModelForQuestionAnswering

model = ORTModelForQuestionAnswering.from_pretrained("optimum/roberta-base-squad2")
tokenizer = AutoTokenizer.from_pretrained("deepset/roberta-base-squad2")

onnx_qa = pipeline("question-answering", model=model, tokenizer=tokenizer)

question = "What's my name?"
context = "My name is Philipp and I live in Nuremberg."
pred = onnx_qa(question, context)
```

å¦‚æœä½ ä½¿ç”¨è‹±ç‰¹å°” CPUï¼Œå¯ä»¥æŸ¥çœ‹ ğŸ¤— [Optimum Intel](https://huggingface.co/docs/optimum/intel/index)ï¼Œæ”¯æŒå„ç§å‹ç¼©æŠ€æœ¯ï¼ˆé‡åŒ–ã€å‰ªæã€çŸ¥è¯†è’¸é¦ï¼‰å’Œå°†æ¨¡å‹è½¬æ¢ä¸º [OpenVINO](https://huggingface.co/docs/optimum/intel/inference) æ ¼å¼ä»¥æé«˜æ¨ç†æ€§èƒ½çš„å·¥å…·ã€‚
